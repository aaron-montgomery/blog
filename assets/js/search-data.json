{
  
    
        "post0": {
            "title": "Penney's Game, pt. 3",
            "content": "library(ggplot2) library(dplyr) library(tidyr) library(stringr) options(repr.plot.width=10, repr.plot.height=6) . Attaching package: &#39;dplyr&#39; The following objects are masked from &#39;package:stats&#39;: filter, lag The following objects are masked from &#39;package:base&#39;: intersect, setdiff, setequal, union . . The Story So Far . We&#39;ll begin with a quick recap. . Penney&#39;s Game, loosely, is a game about waiting for particular sequences of coin throws inside a larger sequence of fair coin throws. In our first foray into the topic, we examined head-to-head matchups between sequences of three coins. That is, you and a friend both choose a sequence of three coin flips, and the winner is determined by whose sequence appears first. . The first surprising result about this game -- or at least, surprising if your intuition is anything like mine was -- is that this game is far from fair, and that the win probabilities depend highly on which sequences are chosen. . (NB: I&#39;ve collapsed the code below since it showed up in a nearly-identical form in a previous post, but you can view it by clicking the &quot;Show Code&quot; button if you&#39;re interested.) . #- # This function will take two strings of coin flips and return the one that occurs # first in a long sequence of coin flips. penney &lt;- function(p1, p2){ # p1 and p2 are strings like &quot;HHT&quot;, &quot;HTT&quot; coins &lt;- nchar(p1) # record how many coins are in the sequences to be compared p1 &lt;- unlist(str_split(p1, &quot;&quot;)) # split string into vector of length equal to coins p2 &lt;- unlist(str_split(p2, &quot;&quot;)) flips &lt;- sample(c(&quot;H&quot;, &quot;T&quot;), coins, replace = T) # initialize flips with first coin flips # on each pass of the while loop, push the first n-1 coins forward in the list, # then sample a new coin for the nth one; for instance, HTH becomes TH? where # ? is the result of a new coin flip while(!all(p1 == flips) &amp; !all(p2 == flips)){ flips[1:(coins - 1)] &lt;- flips[2:coins] flips[coins] &lt;- sample(c(&quot;H&quot;, &quot;T&quot;), 1) } ifelse(all(p1 == flips), paste0(p1, collapse = &quot;&quot;), paste0(p2, collapse = &quot;&quot;)) # output winning sequence } #- # this helper function takes two coin sequences, simulates Penney&#39;s game many # times, and returns the proportion of times the *first* sequence wins penney_wrapper &lt;- function(p1, p2){ mean(replicate(1e4, penney(p1, p2)) == p1) } # The next function produces a vector of appropriate length enumerating all # possible sequences of coin flips of that length; for instance, ht_iterator(2) # will return the vector c(&quot;HH&quot;, &quot;HT&quot;, &quot;TH&quot;, &quot;TT&quot;). This works recursively by # calling ht_iterator(k-1), duplicating it, and appending each of H and T to # one of the duplicates. ht_iterator &lt;- function(k){ if(k == 1) { return(c(&quot;H&quot;, &quot;T&quot;)) } else { ht_iterator(k-1) %&gt;% rep(each = 2) %&gt;% paste0(c(&quot;H&quot;, &quot;T&quot;)) } } results &lt;- ht_iterator(3) %&gt;% combn(2) %&gt;% t() %&gt;% as.data.frame() # makes a data frame with all combinations of sequences names(results) &lt;- c(&quot;p1&quot;, &quot;p2&quot;) results &lt;- results %&gt;% rowwise() %&gt;% mutate(p1_wins = penney_wrapper(p1, p2)) %&gt;% mutate(p1_wins = round(p1_wins, 2)) results &lt;- tibble( p1 = results$p2, p2 = results$p1, p1_wins = 1 - results$p1_wins ) %&gt;% bind_rows(results) # since we simulated only half of the matchups, we can just mirror # the results to flip p1 and p2 rather than resimulating results %&gt;% ggplot(aes(x = p1, y = p2, fill = p1_wins)) + geom_tile() + geom_text(aes(label = p1_wins), size = 6) + scale_fill_gradient2(low = &quot;yellow&quot;, mid = &quot;white&quot;, high = &quot;cyan&quot;, midpoint = 0.5) + labs(title = &quot;Head-to-head matchups: Penney&#39;s Game&quot;, subtitle = &quot;10K trials per matchup combination&quot;, fill = &quot;p1 win %&quot;) + theme(text = element_text(size = 20)) . . Next, we explored a non-competitive variant of this game in which a player waits for a sequence to appear. Like before, the result that many find counterintuive at first -- and I very much include myself in that group -- is that the expected wait time depends on the sequence chosen. (As before, I&#39;ve obscured the code since it appeared in a nearly-identical form in a previous post, but you can click the button below if you want to see it.) . penney_wait &lt;- function(p1){ # p1 is a string like &quot;HHT&quot; coins &lt;- nchar(p1) # keep track of how many coins needed p1 &lt;- unlist(str_split(p1, &quot;&quot;)) # split string into vector w/ length == coins flips &lt;- sample(c(&quot;H&quot;, &quot;T&quot;), coins, replace = T) # initialize flips with first collection of coin flips num_flips &lt;- coins # keep track of how many flips have happened # on each pass of the while loop, push the first n-1 coins forward in the list, # then sample a new coin for the nth one; for instance, HTH becomes TH? where # &quot;?&quot; is the result of a new coin flip while(!all(p1 == flips)){ flips[1:(coins - 1)] &lt;- flips[2:coins] flips[coins] &lt;- sample(c(&quot;H&quot;, &quot;T&quot;), 1) num_flips &lt;- num_flips + 1 } num_flips # output number of flips required to find sequence } tibble(sequence = ht_iterator(3)) %&gt;% rowwise() %&gt;% mutate(avg_wait = mean(replicate(1e4, penney_wait(sequence)))) %&gt;% ggplot(aes(x = sequence, y = avg_wait)) + geom_bar(stat = &quot;identity&quot;) + labs(title = &quot;Monte Carlo simulations of wait times&quot;, subtitle = &quot;10,000 total trials per sequence&quot;, y = &quot;Average wait time&quot;) + theme(text = element_text(size = 20)) + coord_flip() . . Of course, there are good reasons for these phenomena. As with any interesting mathematical question, you can go through the calculations to verify these facts. And yet, in my experience, most people (mathematicians and non-mathematicians alike) crave an intuitive understanding of the problems that lives on a different plane from the calculations and proof. . When I was first becoming accommodated with Penney&#39;s Game, I reconciled the two graphs above by concluding that coin sequences are naturally tiered. From the graph of average wait times, we see a suggested hierarchy: . ranking sequences . slow | HHH, TTT | . medium | HTH, THT | . fast | HHT, HTT, THH, TTH | . Indeed, from examining the graph of head-to-head win probabilities, we see that this is indeed the case; all &quot;fast&quot; sequences carry at least a $50 %$ win rate over all &quot;medium&quot; and &quot;slow&quot; sequences, and all &quot;medium&quot; sequences also hold this over the &quot;slow&quot; sequences. . I found this to be quite satisfying, and it led me to wonder: In Penney&#39;s Game, do sequence structures always give rise to this type of hierarchy of both expected values and competitive advantages? . The Incorrect Answer . It&#39;s probably clear where I&#39;m heading with this, right? This conjecture seemed completely reasonable to me. I assumed that sequences with lower expected wait times would always have competitive advantages (or at least be fair propositions) against sequences with higher expected wait times. . The Correct Answer . There are no hierarchies here, because Penney&#39;s Game is a place where reasonable conjectures go to die. Let&#39;s have a look at the expected wait times for the four-coin version of the game: . # penney_wait() appears in the second collapsed code block above tibble(sequence = ht_iterator(4)) %&gt;% rowwise() %&gt;% mutate(avg_wait = mean(replicate(1e4, penney_wait(sequence)))) %&gt;% ggplot(aes(x = sequence, y = avg_wait)) + geom_bar(stat = &quot;identity&quot;) + labs(title = &quot;Monte Carlo simulations of wait times&quot;, subtitle = &quot;10,000 total trials per sequence&quot;, y = &quot;Average wait time&quot;) + theme(text = element_text(size = 20)) + coord_flip() + geom_hline(yintercept = c(16, 18, 20, 30), lty = 2, lwd = 2, color = &quot;skyblue&quot;) . One of the challenges of the Monte Carlo approach is that the bars in the graph fluctuate a bit based on the random simulations, and it can be tough to determine what the exact values are visually. However, by a theorem (one that has not appeared in these notes... yet), the true expected wait times of each of these sequences is an even integer. . That fact is enough to deduce the true expected wait times despite the fluctuations from simulations. Vertical lines on the graph above have been superimposed at positions $x = 16, 18, 20, 30$. That is: . sequences expected wait time tier . TTTT, HHHH | 30 | slowest | . HTHT, THTH | 20 | med. slow | . TTHT, THTT, THHT, HTTH, HTHH, HHTH | 18 | med. fast | . TTTH, TTHH, THHH, HTTT, HHTT, HHHT | 16 | fastest | . If a hierarchy exists, this is what it would have to be. . Next, let&#39;s pit all the &quot;fastest&quot; group coins against the &quot;med. fast&quot; group coins competitively. There&#39;s nothing special about this particular choice of tiers to compare, other than that it gives us lots of combinations to pit against each other. . # penney_wrapper() four_coin_results &lt;- expand.grid(c(&quot;TTTH&quot;, &quot;TTHH&quot;, &quot;THHH&quot;, &quot;HTTT&quot;, &quot;HHTT&quot;, &quot;HHHT&quot;), c(&quot;TTHT&quot;, &quot;THTT&quot;, &quot;THHT&quot;, &quot;HTTH&quot;, &quot;HTHH&quot;, &quot;HHTH&quot;), stringsAsFactors = FALSE) %&gt;% as.data.frame() %&gt;% rename(&quot;p1&quot; = &quot;Var1&quot;, &quot;p2&quot; = &quot;Var2&quot;) %&gt;% mutate_all(as.character()) four_coin_results &lt;- four_coin_results %&gt;% rowwise() %&gt;% mutate(p1_wins = penney_wrapper(p1, p2)) %&gt;% mutate(p1_wins = round(p1_wins, 2)) four_coin_results %&gt;% ggplot(aes(x = p1, y = p2, fill = p1_wins)) + geom_tile() + geom_text(aes(label = p1_wins), size = 6) + scale_fill_gradient2(low = &quot;yellow&quot;, mid = &quot;white&quot;, high = &quot;cyan&quot;, midpoint = 0.5) + labs(title = &quot;Head-to-head matchups: Penney&#39;s Game&quot;, subtitle = &quot;10K trials per matchup combination&quot;, fill = &quot;p1 win %&quot;, x = &quot;p1 (fastest group)&quot;, y = &quot;p2 (med. fast group)&quot;) + theme(text = element_text(size = 20)) . A few coins in the &quot;fastest&quot; group do indeed dominate those in the &quot;med. fast&quot; group -- but not all of them. HHTT, HTTT, THHH, and TTTH all have a competitive disadvantage against at least one coin from the allegedly lower tier. . Goodbye, Reasonable Conjecture... we hardly knew ye. . The Big Lesson . The most obvious lesson is that Penney&#39;s Game is weird -- which, sure. That&#39;s at least the third time we&#39;ve learned that particular lesson. More generally, we might use this as a reminder that probability is home to lots of counterintuitive results. . But I think the most important part of this story comes back to the idea of a mathematical laboratory. The Penney&#39;s Game saga so far naturally invites several questions for further exploration, such as: . If I know the sequence chosen by my opponent, how do I build the sequence with the best advantage over it? | Can a shorter sequence (like TTT) ever have a faster wait time than a longer sequence (like HTHH)? We&#39;ve actually seen that the answer is no for sequences of length 3 and 4, but what about longer sequences? | . | Is there any kind of pattern in the expected wait times as the sequence lengths increase? | Can a longer sequence ever have a direct head-to-head competitive advantage over a shorter sequence? | . Francis Su writes in his wonderful book Mathematics for Human Flourishing about what it means to be a mathematical explorer: . Exploration cultivates an expectation of enchantment. Explorers are excited by the thrill of finding the unexpected, especially things weird and wonderful. It&#39;s why hikes through unfamiliar terrain entice us, why unexplored caves beckon to us, why the strange creatures of the deep-sea ocean floor fascinate us -- what else may be lurking down there? There&#39;s similar enchantment to be found in the zoo of strange discoveries in math. . Anyone with any level of expertise in probability (including none), and the ability to write code, can engage in investigations like those above. Thanks to Monte Carlo simulations, the joy of exploration in Penney&#39;s Game can suddenly become accessible as a new class of explorers are invited to broaden the scope of who can partake in the beauty of mathematics. This is the way that Monte Carlo simulations can function as a mathematical laboratory; they can be used as a vessel for exploration and discovery. . If I could change one thing about my own experience working toward my PhD in Mathematics, I would have embraced Monte Carlo simulations much earlier in my career. I was able to construct them as a graduate student, but since I had never considered them in a particularly structured way, I tended not to reach for that tool when working on problems. In hindsight, I think engaging more in simulations early in my career could have sharpened my intuition, confirmed difficult theoretical calculations, and maybe even suggested new theorems to try to prove. . To Be Continued... . And yet, Monte Carlo simulations are also limited in scope. The ability of simulations to suggest conjectures is profound; their ability to prove those conjectures is flimsy at best. They typically fall short of showing why things are the way they are. . In Penney&#39;s Game, some of the &quot;why&quot; questions might include: Why do some sequences have competitive advantages against each other? What are the structural considerations of sequences that inform their expected wait times or competitive advantages? . What are the exact win probabilities between sequences that are reflected as mere shadows in Monte Carlo simulations above? | Why can different sequences have different expected wait times? | Why are the expected wait times of any sequence always even integers? | . Questions like these require good old-fashioned mathematical arguments. Those are up next. .",
            "url": "https://aaron-montgomery.github.io/blog/probability/simulation/monte-carlo/penneys-game/counterintuitive/2022/09/23/penneys-game-3.html",
            "relUrl": "/probability/simulation/monte-carlo/penneys-game/counterintuitive/2022/09/23/penneys-game-3.html",
            "date": " • Sep 23, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Penney's Game, pt. 2",
            "content": "Reimagining Penney&#39;s Game . We&#39;ll start with a modified version of Penney&#39;s Game. As before, you&#39;re playing a game involving a sequence of coin flips -- but this time, you&#39;re playing it alone. You have chosen a sequence of three coin flips (let&#39;s say, HTH), and you&#39;re waiting to see how long it will take to encounter your sequence. . If your sequence of coins was . THTTHTH . then it would have taken you 7 coin flips to find the sequence you wanted. Clearly, if you&#39;re lucky, you might encounter your sequence in the first three flips; on the other hand, if you&#39;re unlucky, it could take a hundred coin flips for you to see your sequence. The question is: does the choice of sequence affect how long it will take you to encounter it, on average? . The Incorrect Answer . Much like the first Penney&#39;s Game question, a little knowledge is a dangerous thing. In a fixed collection of 3 coin flips, each sequence of length three has a $ frac{1}{2^3} = frac 1 8$ chance of occurring. When I first heard this question, it seemed like a pretty clear application of a geometric random variable, and since the expected wait time for a $ operatorname{Geometric(p)}$ random variable is $1/p$, it made sense to me that each sequence might have the same wait time. . And yet... . library(ggplot2) library(dplyr) library(tidyr) library(stringr) options(repr.plot.width=10, repr.plot.height=6) . Attaching package: &#39;dplyr&#39; The following objects are masked from &#39;package:stats&#39;: filter, lag The following objects are masked from &#39;package:base&#39;: intersect, setdiff, setequal, union . . penney_wait &lt;- function(p1){ # p1 is a string like &quot;HHT&quot; coins &lt;- nchar(p1) # keep track of how many coins needed p1 &lt;- unlist(str_split(p1, &quot;&quot;)) # split string into vector w/ length == coins flips &lt;- sample(c(&quot;H&quot;, &quot;T&quot;), coins, replace = T) # initialize flips with first collection of coin flips num_flips &lt;- coins # keep track of how many flips have happened # on each pass of the while loop, push the first n-1 coins forward in the list, # then sample a new coin for the nth one; for instance, HTH becomes TH? where # &quot;?&quot; is the result of a new coin flip while(!all(p1 == flips)){ flips[1:(coins - 1)] &lt;- flips[2:coins] flips[coins] &lt;- sample(c(&quot;H&quot;, &quot;T&quot;), 1) num_flips &lt;- num_flips + 1 } num_flips # output number of flips required to find sequence } # The next function produces a vector of appropriate length enumerating all # possible sequences of coin flips of that length; for instance, ht_iterator(2) # will return the vector c(&quot;HH&quot;, &quot;HT&quot;, &quot;TH&quot;, &quot;TT&quot;). This works recursively by # calling ht_iterator(k-1), duplicating it, and appending each of H and T to # one of the duplicates. ht_iterator &lt;- function(k){ if(k == 1) { return(c(&quot;H&quot;, &quot;T&quot;)) } else { ht_iterator(k-1) %&gt;% rep(each = 2) %&gt;% paste0(c(&quot;H&quot;, &quot;T&quot;)) } } tibble(sequence = ht_iterator(3)) %&gt;% rowwise() %&gt;% mutate(avg_wait = mean(replicate(1e4, penney_wait(sequence)))) %&gt;% ggplot(aes(x = sequence, y = avg_wait)) + geom_bar(stat = &quot;identity&quot;) + labs(title = &quot;Monte Carlo simulations of wait times&quot;, subtitle = &quot;10,000 total trials per sequence&quot;, y = &quot;Average wait time&quot;) + theme(text = element_text(size = 20)) + coord_flip() . Of course, I wasn&#39;t thinking through the fact that this clearly isn&#39;t a geometric random variable of that structure, since we don&#39;t need the sequences to fit inside predetermined blocks of three. In the above example, our sequence of coin flips was: . THT|THT|H..| . Adopting the geometric perspective would be tantamount to requiring that the sequences occur strictly between the vertical bars -- but that doesn&#39;t need to happen here. . The Correct Answer . As we can see clearly from the graph, different sequences have different average wait times, and it follows that something about the non-geometricness of the process is the root cause. However, it may not be obvious at first what it is about being non-geometric would cause this discrepancy between sequences. As suggested by the graphs above, there are three tiers of expected wait times in the Penney&#39;s Game problem with three coins: . coin avg. wait time . HHH, TTT | 14 | . HTH, THT | 10 | . HHT, HTT, THH, TTH | 8 | . But why? . The simulation clearly shows that there&#39;s a difference in wait times, but it doesn&#39;t do much to illuminate the reason for it. For that, we need to think carefully about what happens when we&#39;re chasing a sequence and we don&#39;t see what we are hoping for. We will illustrate this concept with some diagrams, starting with HHH. . . The above diagram illustrates the collection of possible things that can occur when we&#39;re trying to complete the HHH sequence. When we begin, we want to see three flips of heads in a row. Each successive flip of H moves us to the right along the diagram. But if at any point we miss and see a T, we lose all our progress and have to start over from scratch. . . Through this perspective, we can start to see why HTH is a fundamentally friendlier sequence to chase than HHH. If we have obtained the first H in the sequence, we hope that the next throw will be a T; however, if it isn&#39;t, then it&#39;s an H instead. This means that when we&#39;re at step 1 of 3 in completing the sequence, missing the coin we hope to see just means we remain at step 1 instead of resetting all the way back to the start. . . The HHT case is even more favorable; here, once we see two consecutive H&#39;s, there&#39;s no way to lose our progress; we&#39;re just waiting for the eventual trailing T. . Examining these three cases carefully gives a useful perspective: the secret is in the blue backtracking arrows. Let&#39;s recap what blue arrows we&#39;ve seen in the diagrams above: . HHH . one loop | two blue arrows (one long, one short) | . | HTH . two loops | one long blue arrow | . | HHT . two loops | one short blue arrow | . | . This shows us that HHH has the most backtracking, HTH has the second most, and HHT has the least, which corresponds to their respective average wait times. This should make sense; the wait time will be determined not by what happens when you get the coin throws you want, but by how far your progress is set back when you don&#39;t get what you want. . (NB: Hopefully, this argument makes it clear why some wait times are longer than others; however, it falls short of establishing that those wait times are exactly 8, 10, and 14. I&#39;ll save those details for another post.) . To be continued.... . In the first post on Penney&#39;s Game, we saw that different sequences have various advantages over each other in a head-to-head matchup; now, we&#39;ve also seen that different sequences have different expected wait times in isolation. To me, these two things are already plenty strange -- and yet, somehow, things will get even more bizarre when we put a fourth coin in the mix. .",
            "url": "https://aaron-montgomery.github.io/blog/probability/simulation/monte-carlo/penneys-game/counterintuitive/2022/08/10/penneys-game-2.html",
            "relUrl": "/probability/simulation/monte-carlo/penneys-game/counterintuitive/2022/08/10/penneys-game-2.html",
            "date": " • Aug 10, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Penney's Game, pt. 1",
            "content": "Penney&#39;s Game . The setup: you&#39;re playing a game with a friend involving coin flips. Both you and your friend choose a sequence of three coin flips; let&#39;s say that you choose THH, and your friend chooses HHT. A neutral referee flips a single coin repeatedly until one of you encouters the sequence you&#39;re waiting for. So, for instance, if the sequence of coin flips is . THTTHTHTHH . then you would win the game, because THH occurred before HHT ever did. The question is: is this a fair game? Or, does one player have an advantage over the other? . The Incorrect Answer . This question, introduced by Walter Penney (hence the name), is a great question to trip up some people who know some probability. When I first heard this question, the answer seemed obvious -- of course it&#39;s fair! After all, I knew that in a sequence of of three isolated coin flips, both THH and HHT (and any other sequence of coin flips) had a $ frac 1 2 cdot frac 1 2 cdot frac 1 2 = frac 1 8$ chance of appearing. That claim is of course true, but in the spirit of a Mathematical Laboratory, let&#39;s take the opportunity to visualize it with some quick Monte Carlo simulations: . library(ggplot2) library(dplyr) library(tidyr) library(stringr) options(repr.plot.width=10, repr.plot.height=6) . Attaching package: &#39;dplyr&#39; The following objects are masked from &#39;package:stats&#39;: filter, lag The following objects are masked from &#39;package:base&#39;: intersect, setdiff, setequal, union . . replicate(1e5, paste0(sample(c(&quot;H&quot;, &quot;T&quot;), 3, replace = T), collapse = &quot;&quot;)) %&gt;% tibble(sequence = .) %&gt;% arrange(sequence) %&gt;% ggplot(aes(x = sequence)) + geom_bar(stat = &quot;count&quot;) + labs(title = &quot;Monte Carlo simulations of flipping 3 coins&quot;, subtitle = &quot;10,000 total trials&quot;) + theme(text = element_text(size = 20)) . As expected, we encounter each of the 8 distinct sequences with approximately equal frequency, so we haven&#39;t gone astray yet. The problem arises when we try to extend that logic to the actual game described up above; in this case, we&#39;re waiting for one of two sequences to appear instead of just stopping after 3 flips and logging the outcomes. Here are the simulations for the THH vs. HHT game we originally described: . # first in a long sequence of coin flips. penney &lt;- function(p1, p2){ # p1 and p2 are strings like &quot;HHT&quot;, &quot;HTT&quot; p1 &lt;- unlist(str_split(p1, &quot;&quot;)) # split string into vector of length 3 p2 &lt;- unlist(str_split(p2, &quot;&quot;)) flips &lt;- sample(c(&quot;H&quot;, &quot;T&quot;), 3, replace = T) # initialize flips with first 3 coin flips # on each pass of the while loop, push the first two coins forward in the list, # then sample a new coin for the third one; for instance, HTH becomes TH? where # ? is the result of a new coin flip while(!all(p1 == flips) &amp; !all(p2 == flips)){ flips[1:2] &lt;- flips[2:3] flips[3] &lt;- sample(c(&quot;H&quot;, &quot;T&quot;), 1) } ifelse(all(p1 == flips), paste0(p1, collapse = &quot;&quot;), paste0(p2, collapse = &quot;&quot;)) # output winning sequence } . replicate(10000, penney(&quot;HHT&quot;, &quot;THH&quot;)) %&gt;% tibble(winner = .) %&gt;% arrange(winner) %&gt;% ggplot(aes(x = winner)) + geom_bar(stat = &quot;count&quot;) + labs(title = &quot;Monte Carlo simulations of Penney&#39;s Game&quot;, subtitle = &quot;10,000 total trials: HHT vs. THH&quot;) + theme(text = element_text(size = 20)) . It is immediately clear that this is not a fair game. . The Correct Answer . Once we see that the game is not fair, we should ask why. For these particular two coin flips, there&#39;s a good argument of why HHT should beat THH just one out of four times: the only way that HHT can win is if the first two coin flips are both heads. Otherwise, there&#39;s some first occurrence of two H&#39;s in the sequence, and that first occurrence is necessarily preceded by a T. The probability that this occurs is $ frac 1 2 cdot frac 1 2 = frac 1 4$, which is why HHT won about 2500 times out of our 10,000 simulations above. . After we understand this head-to-head matchup, the obvious question is: what about all possible head-to-head matchups? To see, we&#39;ll return to the &quot;laboratory&quot; by simulating every possible matchup of all three sequences and generating a heatmap of the results. . # times, and returns the proportion of times the *first* sequence wins penney_wrapper &lt;- function(p1, p2){ mean(replicate(1e4, penney(p1, p2)) == p1) } results &lt;- c(&quot;HHH&quot;, &quot;HHT&quot;, &quot;HTH&quot;, &quot;HTT&quot;, &quot;THH&quot;, &quot;THT&quot;, &quot;TTH&quot;, &quot;TTT&quot;) %&gt;% combn(2) %&gt;% t() %&gt;% as.data.frame() # makes a data frame with all combinations of sequences names(results) &lt;- c(&quot;p1&quot;, &quot;p2&quot;) results &lt;- results %&gt;% rowwise() %&gt;% mutate(p1_wins = penney_wrapper(p1, p2)) %&gt;% mutate(p1_wins = round(p1_wins, 2)) results &lt;- tibble( p1 = results$p2, p2 = results$p1, p1_wins = 1 - results$p1_wins ) %&gt;% bind_rows(results) # since we simulated only half of the matchups, we can just mirror # the results to flip p1 and p2 rather than resimulating results %&gt;% ggplot(aes(x = p1, y = p2, fill = p1_wins)) + geom_tile() + geom_text(aes(label = p1_wins), size = 6) + scale_fill_gradient2(low = &quot;yellow&quot;, mid = &quot;white&quot;, high = &quot;cyan&quot;, midpoint = 0.5) + labs(title = &quot;Head-to-head matchups: Penney&#39;s Game&quot;, subtitle = &quot;10K trials per matchup combination&quot;, fill = &quot;p1 win %&quot;) + theme(text = element_text(size = 20)) . This chart reveals some pretty fascinating truths about Penney&#39;s Game. Here are some of the things we learn: . 1. On the whole, Penney&#39;s Game is certainly not fair. . We can see that some matchups have... . slight advantages, like THH over TTT | moderate advantages, like HHT over TTT | overwhelming advantages, like HTT over TTT | . 2. Yet, fair games do exist within Penney&#39;s Game. . No matter which sequence Player 1 picks, Player 2 can pick something that has a $50 %$ chance of beating it; one way to do this is to take the inverse of Player 1&#39;s sequence, like picking TTH in response to HHT. . 3. Rock-Paper-Scissors . On the other hand, if Player 1 picks their sequence first, then no matter what they have picked, Player 1 can pick something that has an advantage over it. This is almost a probabilistic version of rock-paper-scissors; everything is beaten by something. . If Player 1 picks... Then Player 2 should pick... . HHH | THH | . HHT | THH | . HTH | HHT | . HTT | HHT | . THH | TTH | . THT | TTH | . TTH | HTT | . TTT | HTT | . The easy way to remember what Player 2 should pick is: if Player 1 picks $123$, then Player 2 should pick $ overline 212$, where $ overline 2$ denotes the opposite of $2$. . A logical consequence of this is that there are cycles of things that generally defeat each other. For instance: . HHT usually loses to THH, | which usually loses to TTH, | which usually loses to HTT, | which usually loses to HHT. | . In other words, we have a naturally-occurring example of a nontransitive game. . 4. Not all sequences are created equally . Although every sequence is beaten by something, not every sequence beats something else; TTT and HHH have no advantage over any other sequence. The best case scenario for those sequences is a fair game. . 5. Limits of simulations . This turns out to be a decent advertisement for the limits of simulations (as compared to theoretical approaches) as well. Generally, simulations are quite good at conveying big-picture ideas, like the takeaways we identified above. Yet, simulations are not as well-suited to identifying small differences between things. For instance, the theoretical win probability of HHH over THT is $5/12 approx 0.4167$, and the theoretical win probability of HHH over HTT is $2/5 = 0.40$. (See Further Reading for details.) Yet in the heatmap above, these two values are close enough that it is not obvious they represent different numbers under the hood. This can be solved with more replications, of course, but that is an expensive solution that scales poorly as numbers get closer together. . To be continued.... . So far, we&#39;ve seen that Penney&#39;s Game is certainly not fair and that it contains a nontransitive game inside it. However, this isn&#39;t the end of the story; even if we remove the competitive aspect of the game and just focus on what happens with one player, the game still doesn&#39;t behave like you might expect. . Further Reading . You can find a theoretically-driven explanation of Penney&#39;s Game, along with a chart of exact win probabilities (rather than approximations rendered from simulations), here. .",
            "url": "https://aaron-montgomery.github.io/blog/probability/simulation/monte-carlo/penneys-game/counterintuitive/2022/08/05/penneys-game-1.html",
            "relUrl": "/probability/simulation/monte-carlo/penneys-game/counterintuitive/2022/08/05/penneys-game-1.html",
            "date": " • Aug 5, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Hey, Who Invited You?",
            "content": "Coding as a Mathematical Laboratory . I&#39;ve always been a little bit jealous of lab-based sciences like chemistry and physics. To me, there&#39;s something romantic about being able to conduct true experiments in a physical space. Since mathematicians tend to ply our trade in proofs, our &quot;experiments&quot; might be incomplete theorems on a blackboard or half-formed diagrams scrawled on the back of a napkin. We don&#39;t quite have a natural way to &quot;mix some threes with some sevens and add a dash of elevens&quot; in a way that a chemist might do with hydrogen, sulfer, and oxygen. (NB: Chemists should probably not do that.) . Introducing Monte Carlo simulation into my classrooms has been the first thing that has felt to me like a mathematical laboratory. To illustrate what I mean, I&#39;ll tell a story from a recent class: I assigned the following exercise, which was to be completed with Monte Carlo simulations in R. . Suppose you repeatedly draw upper-case letters from a jar without replacement until the first time you get one that is out of order. What is the expected number of draws for this to occur? (Example:if you draw B-E-R-D, you would stop at D, because it was the first letter you drew that didn&#39;t occur after the previous letter in the alphabet. This would count as 4 draws.) Note that it is OK to ignore the case where you draw all the letters in perfect order, because this is rare enough that it will probably not occur even in trillions of simulations. . At the point in the class when I assign this problem, this is intended to be a medium-difficulty coding problem. Students have to put together many of the skills we have gathered by that point, including: . Monte Carlo simulation | indexing of vectors | loops | . This exercise has my favorite homework aesthetic, which is old skills, assembled together in a new way. But although I&#39;ve long thought it was a pretty good exercise, it wasn&#39;t particularly noteworthy or interesting. At least, I didn&#39;t think it was. . My solution to the problem looked something like this: . letters_until_unordered &lt;- function(){ mydraws &lt;- sample(LETTERS) # draw full sample of all 26 letters check &lt;- 2 # the check variable keeps track of how many we have to draw # before seeing one that is out of order; this is what the # function will return while(identical(mydraws[1:check], sort(mydraws[1:check])) &amp; check &lt;= 26){ check &lt;- check + 1 } # loop should continue until vector isn&#39;t same as sorted self check } # note: if we draw all letters in order, this function will return 27 # this won&#39;t happen, even in a simulation, before the heat death of the universe . That function allows us to compute a Monte Carlo estimate of the expected value from the original question: . mean(replicate(10000, letters_until_unordered())) . 2.7305 In one of the early semesters I taught the class, a student correctly solved the question and brought me his code with an interesting question: is the true answer supposed to be Euler&#39;s number? That is, should we expect the Monte Carlo simulations to hover around $e approx 2.718282...$? . The Incorrect Answer . My immediate answer was: No, that must just be a coincidence. . The Correct Answer . I was wrong. The true answer was actually $e$. Well, almost. . (Quick disclaimer: this result is perfectly well-known to humanity; it just wasn&#39;t well-known to me at the time.) . When the student asked me this question, I replicated my answer a few more times to see if I still thought the resemblence to the magic number $2.718282...$ was accidental. . mean(replicate(10000, letters_until_unordered())) . 2.7268 Hmmm... . mean(replicate(10000, letters_until_unordered())) . 2.7183 Very suspicious. Let&#39;s try using more replications for finer accuracy: . mean(replicate(1e5, letters_until_unordered())) . 2.71941 HMMMM..... . mean(replicate(1e6, letters_until_unordered())) # runtime starts to get a little long: ~50 seconds on my machine . 2.717819 That was hard to ignore, and it practically reeked of Euler&#39;s number. This caught me completely off guard; I had written the problem without any idea that $e$ belonged here at all, and yet... here it was. On the one hand, I shouldn&#39;t have been surprised at all. This is just a Thing That Happens with certain famous mathematical constants like $e$ and $ pi$; they frequently show up where they frankly have no business being. . I did find myself surprised, though -- mostly, by the fact that I had encountered an experiment-driven result, which I am not at all accustomed to. I don&#39;t think I would ever have considered this result were it not for this esoteric homework problem I conjured up (and the help of the keen-eyed student). Monte Carlo simulations gave me something that I imagine must feel a little bit like when a physicist can&#39;t explain a bump on a curve, or when a biologist sees a bacterial growth rate that has no known explanation. The next step for that physicist, or that biologist, and for me, was to ask why. . In this way, I was behaving in the most analogous way possible to an experimental scientist. Such a scientist might discover a fascinating new result in a lab; new results yearn for new theories, and it is those theories that are powerful. Theories are what convey existing and lasting knowledge to humanity. The experiments themselves matter only insofar as they suggest or reveal deeper truths; but the experiments themselves are but partial reflections of those truths. This homework problem doesn&#39;t matter; what makes it tick matters. . This also helped crystallize to me an important lesson about the pedagogy of mathematical education. One of my most important missions in my own classrooms has been to transfer as much agency to students as possible. I tend to gravitate strongly toward active learning strategies. My teaching philosophy is fairly simple: anything students do is good, and anything I do is inherently less good. Here, too, I&#39;ve long been jealous of lab sciences; they have a built-in pedagogy to invite students to meaningfully engage, right from the very beginning of study. Teaching a course on Monte Carlo simulations has been the first thing I&#39;ve done that has felt just a little bit like a lab. . And yet, the simulations have also underscored for me the importance of understanding &quot;traditional&quot; mathematics; we can discover wildly interesting results through Monte Carlo simulations, but only mathematical reasoning can deliver the all-important why. Simulations are a wonderful way to augment a traditional probability and statistics curriculum, but I don&#39;t think they should replace it. . Takeaway . Monte Carlo simulations have given me a new way to invite students who can write just a bit of code to experiment and to play in a mathematical space. Inviting students into this style of mathematical &quot;laboratory&quot; has been incredibly rewarding, and it has allowed me to engage students much more fully than I&#39;m typically able to do in introductory probability / statistics classes. It has allowed me to transfer some of the ownership of the subject material to students, which seems to me an unqualified good thing. . However, Monte Carlo simulations are inherently limited in scope. They can give us incredibly interesting results, and they can do so in a much more accessible way than can classical mathematics and statistics. But by themselves, they can never give us that elusive why. . . Wait, what about the &quot;Why&quot; part? . It would be pretty unsatisfying if after all that discussion, we didn&#39;t get around to the proof of the aforementioned result. Once again, to be clear, this result is perfectly well-known; I just hadn&#39;t personally encountered it before. Once the student brought this seeming coincidence to my attention, I immediately set out to try to prove it. . To remind ourselves, here&#39;s the result: . Theorem . Suppose you repeatedly draw upper-case letters from a jar without replacement until the first time you get one that is out of order. The expected number of draws executed in this fashion will be very slightly less than $e$. . Proof . For $k geq 1$, let $A_k$ denote the event that the first $k$ letters drawn are all in order. (That is: $A_1$ is trivial, $A_2$ holds only if the first two letters are drawn in order, and so forth.) If we let $N$ denote random variable of the number of letters drawn until finding one out of sequence, then we note that $$ N = 1 + mathbb 1_{A_1} + mathbb 1_{A_2} + dots + mathbb 1_{A_{26}}$$ where $ mathbb 1_{B}$ denotes the indicator function of event $B$. This is both the most important and least obvious step in the proof; the logic of this step is that $N$ and each $ mathbb 1_{A_k}$ are random variables, meaning they&#39;re functions of some unseen input variable $ omega$. Here, choosing an $ omega$ determines a full ordering of the letters, and once this has been done, a certain number of events $ {A_1, dots, A_n }$ will be fulfilled, making their corresponding indicator variables evaluate to $1$ on that $ omega$. Summing these indicators will therefore count the events, as desired. . The leading $1$ in the above expression is meant to count the one letter we&#39;ll draw that&#39;s out of order and therefore not counted by the indicator variables; technically, this isn&#39;t very well-defined for the case when we draw all 26 letters in order, but that&#39;s a very low-probability event that we were explicitly invited in the problem statement to ignore, so we&#39;ll just accept the fact that $N$ evaluates to $27$ in that case. (I originally added that detail in the problem statement to alleviate a coding nuisance, and it turned out to alleviate a corresponding mathematical nuisance here.) . The decomposition above is useful, because it therefore follows that $$ mathbb E[N] = 1 + mathbb E left[ mathbb 1_{A_1} right] + mathbb E left[ mathbb 1_{A_2} right] + dots + mathbb E left[ mathbb 1_{A_{26}} right] = 1 + sum_{k=1}^{26} mathbb E left[ mathbb 1_{A_k} right].$$ I should stop and explicitly acknowledge that this really, really feels like we&#39;re cheating somehow. The $ {A_k }$ events are definitely not independent; in fact, they&#39;re completely dependent, as $A_{k+1} subset A_k$. (If the first twelve letters were perfectly ordered, then the first eleven were also.) However, independence is not required for linearity of expectation. . From here, we recall that $ mathbb E left[ mathbb 1_B right] = mathbb P(B)$ for any event $B$, which means that we need only to compute $ mathbb P(A_k)$. When drawing $k$ objects, there are $k!$ rearrangements of those objects, of which only $1$ is properly ordered, from which it follows that $ mathbb P(A_k) = frac 1 {k!}$. Hence, $$ mathbb E[N] = 1 + sum_{k=1}^{26} mathbb P left( A_k right) = 1 + sum_{k=1}^{26} frac{1}{k!} = sum_{k=0}^{26} frac{1}{k!}$$ and we notice that this is simply the first few terms of the infinite series $$ sum_{k=0}^{ infty} frac{1}{k!} = e.$$ . So, the true expected value in our original question isn&#39;t quite $e$ -- it&#39;s a number just below it. Specifically, it&#39;s $e - sum_{k=27}^{ infty} frac{1}{k!}$. To see how close this is to $e$, let&#39;s use R to sum the first few terms of the remainder series: . sum(1 / factorial(27:1000)) # there are times that R&#39;s vectorization comes in really handy . 9.52337830230636e-29 Pretty close to $e$, indeed. And while the true answer isn&#39;t actually quite $e$, we were never going to be able to distinguish the true answer from $e$ with only a Monte Carlo simulation. .",
            "url": "https://aaron-montgomery.github.io/blog/probability/simulation/monte-carlo/eulers-number/counterintuitive/2022/07/27/ordered-objects.html",
            "relUrl": "/probability/simulation/monte-carlo/eulers-number/counterintuitive/2022/07/27/ordered-objects.html",
            "date": " • Jul 27, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Let's Make a Deal!",
            "content": "The Monty Hall Problem . The Monty Hall problem has probably achieved cultural saturation at this point, but we&#39;ll start with a quick refresher: . You are playing a game show in which you are shown three numbered doors (1, 2, 3). One of the doors has a prize behind it; the other two do not. You choose a door -- let&#39;s say door 1, though it doesn&#39;t really matter -- after which the host (Monty Hall) opens door 3 to reveal that it does not have the prize. He then offers you the opportunity to switch from door 1, which you chose, to door 2, the only other remaining door. Should you take his offer to switch? (Does it even matter?) . We should stipulate for the record that we mean the traditional collection of &quot;Monty Hall&quot; rules: that is, Monty will always open one door, he does know where the prize is, and he will never reveal a door that has the prize behind it. If you have chosen the prize correctly on the first guess, then he will flip a coin to decide which of the other doors to reveal. . The Incorrect Answer . I was first exposed to this problem in college when a friend posed the question to me as a brain teaser to at a cafeteria table. I gave the only sensible answer, which was to say that since you had two doors, getting the prize was a 50/50 proposition no matter what you chose. I was, of course, dead wrong. . For years, I was slightly embarrassed about this. Not too embarrassed, mind you -- after all, I&#39;m wrong all the time. But this one stung because my friend was on my home turf; at this point in my life, I was pretty sure I was going to try to pursue a PhD in Mathematics and I suspected I might gravitate to the field of Probability Theory. Just like that, I whiffed on a probability question in a semi-public forum. . My shame was lessened over the years when I learned that if nothing else, I wasn&#39;t alone. When Marilyn Vos Savant gave a correct solution to the problem in a 1990 issue of Parade, she received a truckload of letters, many from professional mathematicians, telling her how wrong she was. (She wasn&#39;t wrong, which certainly didn&#39;t help the outrageous rudeness of some of those letters.) In his book Which Door Has the Cadillac: Adventures of a Real Life Mathematician, Andrew Vazsonyi recalls giving the same incorrect &quot;obvious&quot; answer to the problem on his first encounter; perhaps more shockingly, he details an account of discussing the problem with Paul Erdös, who also got the problem wrong and became increasingly irate about it until he was eventually shown a simulation proving what the right answer should be. . The Correct Answer . That right answer is that switching is better. Indeed, staying with your original choice will grant you a $1/3$ chance of winning, and switching will grant a $2/3$ chance of winning. The key detail, of course, is Monty&#39;s knowledge of the prize location and his choice of exactly how to reveal what he knows. There are many ways to see why this is true; the Wikipedia entry for the Monty Hall problem gives many different explanations of many different flavors (and even criticisms of those same explanations). These explanations are great, but to those who aren&#39;t accustomed to long mathematical arguments, they might be less than convincing. . The first explanation of the solution in the Wikipedia article states: . When the player first makes their choice, there is a $2/3$ chance that the car is behind one of the doors not chosen. This probability does not change after the host reveals a goat behind one of the unchosen doors. . This explanation never quite sat right with me. Sasha Volokh expressed my vague concern quite well: . First, it’s clear that any explanation that says something like “the probability of door 1 was 1/3, and nothing can change that...” is automatically fishy:probabilities are expressions of our ignorance about the world, and new information can change the extent of our ignorance. . This is a case where simulations can do us some good. . Simulations . We&#39;ll write Monte Carlo simulations in R to see that by sticking with our original answer, the probability of winning is indeed $1/3$. We will write a function that simulates one full round of the game; then, we&#39;ll replicate() the function many times to determine the probability of winning. Our strategy will be to simulate a round of the full game many times and keep track of how often the game results in a win. . For our first attempt, we&#39;ll recreate a very general version of the game: . the prize can be found behind any of 3 doors | the contestant will pick any of 3 doors | Monty will reveal one door and offer a chance to switch | the contestant will choose to switch or stay depending on the parameter stay | . library(ggplot2) library(dplyr) library(tidyr) options(repr.plot.width=10, repr.plot.height=6) monty_hall &lt;- function(stay){ doors &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) # using ABC instead of 123 due to a quirk in sample() prize_door &lt;- sample(doors, 1) contestant_choice &lt;- sample(doors, 1) reveal_door &lt;- # Monty can&#39;t reveal the door with the prize, union(prize_door, contestant_choice) %&gt;% # nor can he reveal the selected door, so we&#39;ll setdiff(doors, .) %&gt;% # remove those choices from the doors vector sample(1) switch_offer &lt;- setdiff(doors, c(contestant_choice, reveal_door)) ifelse(prize_door == ifelse(stay, contestant_choice, switch_offer), &quot;win&quot;, &quot;lose&quot;) # the function returns the strings &quot;win&quot; and &quot;lose&quot; # when stay is TRUE, check to see if the prize door matches the contestant&#39;s choice # when stay is FALSE, check to see if the prize door matches the door offered in a switch } . Next, we&#39;ll generate 10K trials of the game under each of the two options (switching and staying). We&#39;ll reshape the outcomes just a bit and then plot them. . monty_stay_trials &lt;- replicate(10000, monty_hall(stay = TRUE)) monty_switch_trials &lt;- replicate(10000, monty_hall(stay = FALSE)) . data.frame(stay = monty_stay_trials, switch = monty_switch_trials) %&gt;% pivot_longer(everything(), names_to = &quot;choice&quot;, values_to = &quot;outcome&quot;) %&gt;% ggplot(aes(x = outcome)) + geom_bar(stat = &quot;count&quot;) + facet_wrap(~choice) + labs(title = &quot;Monty Hall Monte Carlo simulation results&quot;, subtitle = &quot;10,000 trials per contestent choice possibility&quot;) + theme(text = element_text(size = 20)) . The simulations show us exactly what we expected; switching is good, staying is bad. We can also easily confirm that the probability of winning by switching is $2/3$: . mean(monty_switch_trials == &quot;win&quot;) . 0.6747 Our answer is close to $2/3$, and the difference between it and $2/3$ is a small statistical fluctuation, as expected. . So, good news: we have confirmed that the correct answer is indeed correct. But can we render any deeper insights from this? Let&#39;s focus on the function in the case when stay == TRUE. In that case, the code looks like this: . monty_hall &lt;- function(stay){ doors &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) prize_door &lt;- sample(doors, 1) contestant_choice &lt;- sample(doors, 1) reveal_door &lt;- union(prize_door, contestant_choice) %&gt;% setdiff(doors, .) %&gt;% sample(1) switch_offer &lt;- setdiff(doors, c(contestant_choice, reveal_door)) ifelse(prize_door == contestant_choice, &quot;win&quot;, &quot;lose&quot;) } . Here, switch_offer doesn&#39;t actually get used at all in the line that returns the function. This means that the switch_offer &lt;- ... line, and the reveal_door &lt;- ... lines, are unused appendages. If we remove them, we&#39;re left with this: . monty_hall &lt;- function(stay){ doors &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) prize_door &lt;- sample(doors, 1) contestant_choice &lt;- sample(doors, 1) ifelse(prize_door == contestant_choice, &quot;win&quot;, &quot;lose&quot;) } . Now, we see that our code has reduced to a simulation that draws two objects independently from a collection of three and checks to see if they&#39;re the same. That probability is clearly $1/3$, and this explanation now aligns perfectly with the Wikipedia explanation that nothing has changed the original probability of $1/3$, whether that explanation is &quot;fishy&quot; or not. . Takeaway . It&#39;s great to be able to use Monte Carlo simulations to confirm a correct answer, but in this case the act of writing a simulation can do something more profound: it can make the why behind the answer just a bit more convincing. Or, at least, it did that for me. I&#39;ve seen (and believed, and produced) many analytical arguments for why switching has a $2/3$ probability, but I never fully believed the Wikipedia explanation until writing code to simulate the game. .",
            "url": "https://aaron-montgomery.github.io/blog/probability/simulation/monte-carlo/monty-hall/counterintuitive/2022/07/14/monty-hall.html",
            "relUrl": "/probability/simulation/monte-carlo/monty-hall/counterintuitive/2022/07/14/monty-hall.html",
            "date": " • Jul 14, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi! I’m Aaron Montgomery, an Associate Professor at Baldwin Wallace University. I received a PhD in Mathematics (specializing in Probability Theory) in 2013 at the University of Oregon under the direction of David Levin. Since then I have been a member of the BW Mathematics and Statistics faculty, and starting in Fall 2022 I will be the coordinator of the new Data Science and Data Analytics programs. . I am always open to collaborating on interesting statistical or data-oriented projects! You can reach me at amontgom (atsymbol) bw.edu. .",
          "url": "https://aaron-montgomery.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://aaron-montgomery.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}